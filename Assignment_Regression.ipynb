{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "Ans. Simple Linear Regression is a method to model the relationship between two variables using a straight line:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "\n",
        "X: independent variable\n",
        "\n",
        "Y: dependent variable\n",
        "\n",
        "β₀: intercept\n",
        "\n",
        "β₁: slope\n",
        "\n",
        "It shows how Y changes as X changes."
      ],
      "metadata": {
        "id": "EA4qXKuJIQBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans.\n",
        "\n",
        "Linearity – Relationship between X and Y is linear.\n",
        "\n",
        "Independence – Observations are independent.\n",
        "\n",
        "Homoscedasticity – Constant variance of errors.\n",
        "\n",
        "Normality – Errors are normally distributed.\n",
        "\n",
        "No multicollinearity – (Only one predictor, so not applicable here)."
      ],
      "metadata": {
        "id": "aiCLzv81Ib9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans. The coefficient m represents the slope — the change in Y for a one-unit increase in X."
      ],
      "metadata": {
        "id": "d3Z-b51aIgXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans. The intercept c is the value of Y when X = 0."
      ],
      "metadata": {
        "id": "4F-v-KwQJMMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans. m=\n",
        "n∑X Y\n",
        " −(∑X) (∑Y)\n",
        "\n",
        "n∑X2−(∑X)2\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑛\n",
        "n = number of data points\n",
        "\n",
        "𝑋\n",
        ",\n",
        "𝑌\n",
        "X,Y = input and output values"
      ],
      "metadata": {
        "id": "FwxBM0sBJRkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans. To minimize the sum of squared errors between the actual and predicted values, giving the best-fitting line through the data."
      ],
      "metadata": {
        "id": "bFGsBsdaOdYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans. R² shows how well the regression line explains the variability in the data.\n",
        "\n",
        "R² = 1 → perfect fit\n",
        "\n",
        "R² = 0 → no explanatory power\n",
        "\n",
        "Higher R² = better model fit"
      ],
      "metadata": {
        "id": "5_Di3bNnOjlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Multiple Linear Regression?\n",
        "\n",
        "Ans. Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "​\n",
        "\n",
        "It estimates how each predictor affects the outcome."
      ],
      "metadata": {
        "id": "OiZGK3uaOnfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans. Simple Linear Regression uses 1 predictor variable.\n",
        "Multiple Linear Regression uses 2 or more predictor variables."
      ],
      "metadata": {
        "id": "Pq5RRjjtOvT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans. Linearity – Relationship between predictors and outcome is linear.\n",
        "\n",
        "Independence – Observations are independent.\n",
        "\n",
        "Homoscedasticity – Constant variance of errors.\n",
        "\n",
        "Normality – Errors are normally distributed.\n",
        "\n",
        "No multicollinearity – Predictors are not highly correlated with each other."
      ],
      "metadata": {
        "id": "7PNmf-aoOynl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans. Heteroscedasticity means the variance of errors is not constant across all levels of the predictors.\n",
        "\n",
        "📉 Effect:\n",
        "\n",
        "It violates regression assumptions.\n",
        "\n",
        "Leads to biased standard errors, making hypothesis tests and confidence intervals unreliable."
      ],
      "metadata": {
        "id": "WbEU1O88PQUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans. To handle high multicollinearity:\n",
        "\n",
        "Remove correlated predictors\n",
        "\n",
        "Combine variables (e.g., via feature engineering)\n",
        "\n",
        "Use dimensionality reduction (e.g., PCA)\n",
        "\n",
        "Apply regularization (e.g., Ridge or Lasso regression)"
      ],
      "metadata": {
        "id": "j5AC5kkUPMPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans. Common techniques:\n",
        "\n",
        "One-Hot Encoding – Creates binary columns for each category.\n",
        "\n",
        "Label Encoding – Assigns numeric codes to categories.\n",
        "\n",
        "Ordinal Encoding – For categories with natural order.\n",
        "\n",
        "Target Encoding – Uses mean of target variable for each category."
      ],
      "metadata": {
        "id": "UbpXmfpcPURA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans. Interaction terms show how the effect of one predictor on the outcome changes depending on another predictor.\n",
        "They capture combined effects that individual variables alone may miss."
      ],
      "metadata": {
        "id": "V1C8Vzs4PXzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans. In Simple Linear Regression, the intercept is the expected value of Y when X = 0.\n",
        "\n",
        "In Multiple Linear Regression, the intercept is the expected value of Y when all predictors = 0, which may or may not be meaningful depending on the context."
      ],
      "metadata": {
        "id": "5zyITMMUPbsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans. The slope shows how much the dependent variable (Y) changes for a one-unit increase in the independent variable (X).\n",
        "\n",
        "It directly affects predictions by determining the direction (positive/negative) and magnitude of the relationship."
      ],
      "metadata": {
        "id": "hQ9LvO51PpWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans. The intercept represents the expected value of the dependent variable (Y) when all independent variables are zero.\n",
        "\n",
        "It provides a baseline or starting point for understanding the relationship, though it may not always be meaningful in real-world context."
      ],
      "metadata": {
        "id": "YfFFCszXPrdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "Ans. Limitations of using R² alone:\n",
        "\n",
        "Doesn't indicate model accuracy – High R² doesn't mean good predictions.\n",
        "\n",
        "Can be misleading with more variables – R² always increases with more predictors.\n",
        "\n",
        "Doesn’t detect overfitting – A complex model may fit training data well but perform poorly on new data.\n",
        "\n",
        "Ignores model assumptions – R² doesn’t check for linearity, normality, etc."
      ],
      "metadata": {
        "id": "zo1A1750UsyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans. A large standard error means the coefficient estimate is less precise, suggesting:\n",
        "\n",
        "High variability in the data\n",
        "\n",
        "Possible multicollinearity\n",
        "\n",
        "Weak relationship between predictor and outcome\n",
        "\n",
        "It reduces confidence in the reliability of that coefficient."
      ],
      "metadata": {
        "id": "5_OoVAeZUugh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans. Identification:\n",
        "In residual plots, heteroscedasticity appears as a funnel shape — residuals spread out or narrow as predicted values increase.\n",
        "\n",
        "Importance:\n",
        "It violates regression assumptions and leads to:\n",
        "\n",
        "Biased standard errors\n",
        "\n",
        "Unreliable p-values and confidence intervals\n",
        "\n",
        "Misleading conclusions about predictors\n",
        "\n",
        "Addressing it improves model validity and inference."
      ],
      "metadata": {
        "id": "n_VEeOl1VAB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "Ans. It means the model includes irrelevant predictors that don't improve performance.\n",
        "\n",
        "High R²: Total variance explained looks good.\n",
        "\n",
        "Low adjusted R²: Penalizes useless variables → true predictive power is lower.\n",
        "🧠 Interpretation: Model may be overfitting."
      ],
      "metadata": {
        "id": "UUNKyWXBVR8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans. Scaling is important because it:\n",
        "\n",
        "Ensures fair comparison of coefficients (especially when units differ).\n",
        "\n",
        "Improves numerical stability and convergence in algorithms.\n",
        "\n",
        "Helps regularization techniques (like Ridge or Lasso) perform correctly.\n",
        "\n",
        "✅ Especially useful when variables vary widely in range."
      ],
      "metadata": {
        "id": "DSjQhvfGVWPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "\n",
        "Ans."
      ],
      "metadata": {
        "id": "vN7jP-IuWNrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans.Polynomial Regression is a type of regression where the relationship between the independent variable X and dependent variable Y is modeled as an nth-degree polynomial:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "\n",
        "It allows modeling non-linear relationships using a linear model framework."
      ],
      "metadata": {
        "id": "S8pe3DEaWQl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "\n",
        "Ans.Polynomial regression is used when the relationship between variables is non-linear, but you still want to use a linear model with transformed features (like\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        " ).\n",
        "\n",
        "👉 Used when data shows a curved trend that simple linear regression can’t capture."
      ],
      "metadata": {
        "id": "8d-i6cCdWiQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "Ans. The general equation for polynomial regression is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y: Dependent variable\n",
        "\n",
        "𝑋\n",
        "X: Independent variable\n",
        "\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        " : Coefficients\n",
        "\n",
        "𝑛\n",
        "n: Degree of the polynomial\n",
        "\n",
        "𝜀\n",
        "ε: Error term"
      ],
      "metadata": {
        "id": "ZmeJZZGMWm0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans. Yes, polynomial regression can be applied to multiple variables by adding squared, cubed, and interaction terms for those variables.\n",
        "\n",
        "This allows modeling complex, non-linear relationships in multivariate data."
      ],
      "metadata": {
        "id": "Gk2hyVGpXUiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  What are the limitations of polynomial regression?\n",
        "\n",
        "Ans. Limitations of polynomial regression:\n",
        "\n",
        "Overfitting – High-degree polynomials may fit noise, not the true pattern.\n",
        "\n",
        "Poor extrapolation – Unstable outside the data range.\n",
        "\n",
        "Complexity – Becomes hard to interpret with many terms.\n",
        "\n",
        "Multicollinearity – Polynomial terms can be highly correlated.\n",
        "\n",
        "Sensitive to outliers – Can distort the curve significantly."
      ],
      "metadata": {
        "id": "t_JfHn6-XfHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans. To evaluate model fit and choose the right polynomial degree, use:\n",
        "\n",
        "R² and Adjusted R² – Show how well the model explains the variance.\n",
        "\n",
        "Cross-Validation (e.g., k-fold) – Tests performance on unseen data.\n",
        "\n",
        "Mean Squared Error (MSE) / RMSE – Measures average prediction error.\n",
        "\n",
        "AIC / BIC – Penalize model complexity to avoid overfitting.\n",
        "\n",
        "Residual Plots – Visual check for randomness of residuals."
      ],
      "metadata": {
        "id": "qQjEIA_YXiRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans. Visualization is important in polynomial regression because it:\n",
        "\n",
        "Reveals curve fit – Shows how well the model captures data patterns.\n",
        "\n",
        "Detects overfitting/underfitting – Too complex or too simple curves are easy to spot.\n",
        "\n",
        "Illustrates relationships – Helps understand non-linear effects visually.\n",
        "\n",
        "Validates residuals – Residual plots can confirm model assumptions."
      ],
      "metadata": {
        "id": "L78zaTAuXl7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans."
      ],
      "metadata": {
        "id": "Vg5mqhTRXn3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example: degree-2 polynomial\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)  # X must be 2D, e.g., X = X.reshape(-1, 1)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X)\n"
      ],
      "metadata": {
        "id": "JY246YDWZQbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}